# This file is automatically generated by `./configs/benchmarks/multi_task_learning/gen_multi_task_learning.py`.
# Please do not attempt to modify manually.
import torch
import schedulers


config = {
    'runner': None,
    'work_dir': None,
    'epochs': 100,
    # seeds
    'init_seed': None,
    'train_seeds': None,
    'val_seeds': None,
    'test_seed': None,
    # dataset config
    'train_dataset': None,
    'train_dataloader': None,
    'val_dataset': None,
    'val_dataloader': None,
    'test_dataset': None,
    'test_dataloader': None,
    'criterion': None,
    'metric': None,
    # model config
    'model': None,
    # optimizer config
    'optimizer': None,
    'scheduler': {
        'class': torch.optim.lr_scheduler.LambdaLR,
        'args': {
            'lr_lambda': {
                'class': schedulers.lr_lambdas.WarmupLambda,
                'args': {},
            },
        },
    },
}

from runners import SupervisedMultiTaskTrainer
config['runner'] = SupervisedMultiTaskTrainer

# dataset config
from configs.common.datasets.multi_task_learning.multi_mnist import config as dataset_config
config.update(dataset_config)

# model config
from configs.common.models.multi_task_learning.multi_mnist.multi_mnist_lenet5 import model_config_all_tasks as model_config
config['model'] = model_config

# optimizer config
from configs.common.optimizers.multi_task_learning.graddrop import optimizer_config
from configs.common.optimizers.standard import adam_optimizer_config
optimizer_config['args']['optimizer_config'] = adam_optimizer_config
optimizer_config['args']['per_layer'] = False
config['optimizer'] = optimizer_config

# seeds
config['init_seed'] = 75589482
config['train_seeds'] = [14371012, 56341670, 73612867, 90383288, 28709435, 39639390, 241172, 47222990, 48660785, 98256081, 61426011, 42492802, 71453949, 13827168, 99535767, 40704183, 28777416, 73903834, 54286794, 230925, 34514813, 23929773, 67933096, 67467708, 60449886, 97063909, 58598759, 43871363, 6682415, 95274944, 44550736, 39759893, 2548040, 24817986, 18692901, 67947967, 8444209, 77243217, 65644353, 2077138, 92675789, 51720111, 74737468, 44928526, 66528333, 7478935, 2234810, 90977552, 74353755, 40229907, 20712570, 34469023, 34172376, 55824378, 74491944, 10098274, 66011651, 22199656, 60854987, 71996590, 32991064, 58941496, 70873938, 95891869, 49021207, 6816937, 43855496, 86656808, 84518360, 38109637, 55901064, 25375493, 98562793, 25980341, 79492677, 93844685, 49381907, 86449914, 57769305, 1164130, 72434740, 64907759, 60393817, 37218538, 98077007, 21238668, 30405012, 55720884, 20244852, 66493573, 40732154, 22021471, 51194092, 55067708, 51379796, 75615761, 55117875, 85272857, 2666143, 3801378]
config['val_seeds'] = [88831501, 50589615, 88568251, 98549378, 57716783, 43408176, 41177739, 89834975, 8172443, 78610659, 87381208, 75063016, 83626687, 71610630, 43859969, 50343529, 85199907, 26532518, 53567456, 15926953, 31321107, 81811491, 22577909, 49055551, 20847253, 31811271, 49360963, 34751919, 37012568, 91055730, 18288950, 90735412, 66610024, 87889947, 1762209, 70125936, 90855441, 66999147, 64442718, 3393108, 91167143, 1946499, 91434743, 80176666, 9928676, 68904768, 66917065, 10064396, 64143298, 7399188, 85115009, 90785384, 30203677, 98697749, 70224630, 48968054, 78839587, 1528193, 60747952, 76033258, 22744638, 96340165, 16960156, 20442401, 66238922, 49439361, 60895740, 8294201, 30925989, 78265195, 88472967, 33217087, 80723459, 39518307, 34433448, 14010339, 41448837, 86079909, 88622364, 4848240, 63817705, 40594770, 6393889, 17362348, 70079656, 84111980, 80153701, 61013950, 80238086, 53217254, 82364579, 18278939, 2382148, 26684385, 86902534, 26111918, 66155473, 53163983, 91533185, 20867087]
config['test_seed'] = 966166

# work dir
config['work_dir'] = "./logs/benchmarks/multi_task_learning/multi_mnist/multi_mnist_lenet5/graddrop_run_2"
