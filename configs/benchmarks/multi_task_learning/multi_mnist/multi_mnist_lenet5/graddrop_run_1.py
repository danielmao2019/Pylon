# This file is automatically generated by `./configs/benchmarks/multi_task_learning/gen_multi_task_learning.py`.
# Please do not attempt to modify manually.
import torch
import schedulers


config = {
    'runner': None,
    'work_dir': None,
    'epochs': 100,
    # seeds
    'init_seed': None,
    'train_seeds': None,
    'val_seeds': None,
    'test_seed': None,
    # dataset config
    'train_dataset': None,
    'train_dataloader': None,
    'val_dataset': None,
    'val_dataloader': None,
    'test_dataset': None,
    'test_dataloader': None,
    'criterion': None,
    'metric': None,
    # model config
    'model': None,
    # optimizer config
    'optimizer': None,
    'scheduler': {
        'class': torch.optim.lr_scheduler.LambdaLR,
        'args': {
            'lr_lambda': {
                'class': schedulers.lr_lambdas.WarmupLambda,
                'args': {},
            },
        },
    },
}

from runners import SupervisedMultiTaskTrainer
config['runner'] = SupervisedMultiTaskTrainer

# dataset config
from configs.common.datasets.multi_task_learning.multi_mnist import config as dataset_config
config.update(dataset_config)

# model config
from configs.common.models.multi_task_learning.multi_mnist.multi_mnist_lenet5 import model_config_all_tasks as model_config
config['model'] = model_config

# optimizer config
from configs.common.optimizers.multi_task_learning.graddrop import optimizer_config
from configs.common.optimizers.standard import adam_optimizer_config
optimizer_config['args']['optimizer_config'] = adam_optimizer_config
optimizer_config['args']['per_layer'] = False
config['optimizer'] = optimizer_config

# seeds
config['init_seed'] = 11070694
config['train_seeds'] = [38774181, 41511254, 25398200, 41970690, 54476566, 54955599, 65758216, 32992115, 41174564, 84329895, 46132715, 69106466, 64257109, 61303355, 18450623, 26385114, 93355865, 78251462, 44357202, 47810654, 98503462, 68494901, 7072003, 7677378, 11793337, 89053009, 70573017, 10940056, 8243645, 15726367, 40025948, 83091577, 24915097, 47728589, 99061061, 70203584, 30377720, 68945194, 4655687, 99526875, 17881793, 8098041, 23651826, 82343042, 52230330, 665544, 26338552, 73339981, 41336481, 31259865, 31251207, 79243053, 26114525, 86345805, 23854437, 42540173, 18971101, 56048560, 3402861, 1984845, 54344051, 52992, 39749242, 1273967, 17182571, 20601421, 47495652, 9240442, 42552492, 45623652, 43574154, 27975336, 31115772, 79948386, 19215870, 34378380, 94935867, 41987814, 6312093, 18303293, 34571297, 65710336, 88494249, 97435072, 41373230, 91318158, 5612120, 55444565, 4297419, 70490805, 6593576, 30343741, 6141616, 42704822, 44385925, 70855464, 74015688, 52593989, 75207109, 73319640]
config['val_seeds'] = [84635976, 84111707, 23644225, 74296184, 57681472, 19933103, 66750812, 47903299, 44901443, 90151649, 69836749, 21464274, 41774239, 30102698, 18604110, 30705244, 25428733, 97123818, 6367721, 58916768, 96039285, 63111243, 20146233, 48156883, 68069518, 49611032, 27259137, 57579869, 59707436, 67674803, 23804878, 17075067, 95623078, 96334069, 79375708, 27554583, 20953011, 84356647, 16126061, 63757516, 45190798, 94041729, 65679155, 69139209, 24676300, 42171769, 42587907, 20374684, 98547749, 31258513, 4972537, 6537305, 11293021, 76237129, 92650809, 48672857, 20741312, 212793, 58798693, 4665103, 3771089, 40910301, 76201875, 68743652, 64707848, 59447894, 87831659, 62596826, 43723300, 94832802, 74144343, 58634051, 15186529, 53067660, 67800763, 69400512, 27863626, 47217503, 83212411, 72323335, 1986656, 79642626, 3780503, 35087794, 61077587, 79920941, 84862419, 49129329, 64771905, 94019566, 70659479, 3145268, 88959428, 60977030, 24595250, 28010502, 93341513, 65055111, 68639952, 85054505]
config['test_seed'] = 51655991

# work dir
config['work_dir'] = "./logs/benchmarks/multi_task_learning/multi_mnist/multi_mnist_lenet5/graddrop_run_1"
