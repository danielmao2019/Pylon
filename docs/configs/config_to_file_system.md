# ConfigToFile System Documentation

## Overview

The `ConfigToFile` system is Pylon's dictionary-to-Python-file serialization engine that converts config dictionaries into properly formatted Python files with correct imports, formatting, and structure.

## Architecture

### Core Components

1. **`ConfigToFile` class** (`utils/automation/config_to_file.py`)
   - Main serialization engine
   - Handles import collection and formatting
   - Applies formatting rules

2. **`config_seeding.py`** (`utils/automation/config_seeding.py`)
   - Generates seeded config variants
   - Handles training, evaluation, and multi-stage configs
   - Returns serialized strings

3. **`semideepcopy`** (`utils/builders/builder.py`)
   - Deep copy with PyTorch parameter preservation
   - Prevents reference sharing issues

## How It Works

### 1. Config Dictionary Creation

```python
config = {
    'runner': SupervisedSingleTaskTrainer,
    'model': {
        'class': models.change_detection.HANet,
        'args': {}
    },
    'train_dataset': {
        'class': data.datasets.OSCDDataset,
        'args': {
            'data_root': './data/datasets/soft_links/OSCD',
            'transforms_cfg': transforms_cfg(size=(224, 224))
        }
    }
}
```

### 2. Analysis Phase (`_analyze_value`)

The system recursively traverses the config to:
- Detect class/function references
- Collect required imports
- Preserve class objects for later formatting

```python
# Detects that SupervisedSingleTaskTrainer needs:
from runners import SupervisedSingleTaskTrainer

# Detects that models.change_detection.HANet needs:
from models.change_detection import HANet
```

### 3. Formatting Phase (`_format_value`)

Applies strict formatting rules:

**Rule 1: Dictionaries always use newlines**
```python
{
    'class': RandomCrop,
    'args': {
        'size': (224, 224),
        'resize': None,
    },
}
```

**Rule 2: Lists/tuples never use newlines**
```python
[('inputs', 'img_1'), ('inputs', 'img_2'), ('labels', 'change_map')]
```

**Special Case: Tuples containing dicts**
```python
(
    {
        'class': RandomCrop,
        'args': {
            'size': (224, 224),
        },
    },
    [('inputs', 'img_1'), ('inputs', 'img_2')]
)
```

### 4. Import Organization (`_format_imports`)

Imports are organized in a specific order:
1. `typing` imports
2. Standard library imports
3. External packages (numpy, torch, etc.)
4. Project modules

Example output:
```python
from typing import Dict, List, Optional
import os
import torch
from torchvision.transforms import ColorJitter
from data.datasets.change_detection_datasets.bi_temporal.oscd_dataset import OSCDDataset
from models.change_detection.hanet import HANet
```

### 5. Final Assembly

```python
# Auto-generated header
# This file is automatically generated by `./configs/benchmarks/change_detection/gen_bi_temporal.py`.
# Please do not attempt to modify manually.

# Organized imports
from typing import ...
import ...
from ... import ...

# Formatted config
config = {
    # ... beautifully formatted config
}
```

## Function Call Evaluation

One critical aspect is how function calls are handled:

### During Generation
```python
# In the generation script
config['train_dataset']['args']['transforms_cfg'] = transforms_cfg(size=(224, 224))
```

The `transforms_cfg()` function is **called immediately** and returns a dictionary:
```python
{
    'class': Compose,
    'args': {
        'transforms': [
            # ... list of transform tuples
        ]
    }
}
```

### In the Serialized File

The serialized file contains the **evaluated result**, not the function call:
```python
'transforms_cfg': {
    'class': Compose,
    'args': {
        'transforms': [(
            {
                'class': RandomCrop,
                'args': {
                    'size': (224, 224),
                    'resize': None,
                    'interpolation': None,
                },
            },
            [('inputs', 'img_1'), ('inputs', 'img_2'), ('labels', 'change_map')]
        ), ...]
    }
}
```

## Common Pitfalls and Solutions

### 1. Reference Sharing

**Problem**: Multiple configs sharing the same object
```python
# BAD: Shallow copy causes contamination
config.update(train_data_cfg)
```

**Solution**: Use deep copy
```python
# GOOD: Deep copy prevents contamination
config.update(semideepcopy(train_data_cfg))
```

### 2. Module Import Caching

**Problem**: Python caches imported modules in `sys.modules`
```python
# First import gets module state A
from configs.common.datasets.train.oscd_data_cfg import data_cfg

# Later import gets same cached module (possibly modified)
from configs.common.datasets.train.oscd_data_cfg import data_cfg
```

**Solution**: Clear module cache when needed
```python
modules_to_clear = [key for key in sys.modules.keys() 
                   if key.startswith('configs.common')]
for module in modules_to_clear:
    del sys.modules[module]
```

### 3. Class Reference Preservation

**Challenge**: Need to preserve actual class objects during analysis
```python
# Must preserve this as the actual class, not a string
'class': models.change_detection.HANet
```

**Solution**: `_analyze_value` preserves class references while collecting imports

## Integration with Generation Scripts

### Typical Usage Pattern

```python
# 1. Build config dictionary
config = build_config(dataset, model)

# 2. Generate seeded variants (returns serialized strings)
seeded_configs = generate_seeded_configs(
    base_config=config,
    base_seed=relpath,
    base_work_dir=work_dir
)

# 3. Add headers and write to files
for idx, seeded_config in enumerate(seeded_configs):
    final_config = add_heading(seeded_config, generator_path)
    with open(output_path, 'w') as f:
        f.write(final_config)
```

### Seeding System

The seeding system handles three types of configs:

1. **Training Configs**: 
   - Multiple seeds (init, train, val, test)
   - Work directory with run index

2. **Evaluation Configs**:
   - Single test seed
   - No epochs field

3. **Multi-Stage Configs** (e.g., BUFFER):
   - List of stage configs
   - Each stage gets its own seeds

## Benefits of the New System

1. **Self-Contained Files**: No need to trace imports to understand config values
2. **Deterministic**: Same input always produces same output
3. **Debuggable**: Can see exact values without runtime evaluation
4. **Version Control Friendly**: Changes to configs are explicit in diffs
5. **Import Management**: Automatic and correct import generation

## Limitations

1. **File Size**: Serialized files are larger due to expanded values
2. **Readability**: Deeply nested structures can be harder to read
3. **No Dynamic Values**: Cannot use runtime-computed values
4. **Function Calls**: All functions are evaluated during generation

## Future Improvements

1. **Compression**: Optional compression for repeated structures
2. **Comments**: Preserve or generate helpful comments
3. **Validation**: Built-in config validation during serialization
4. **Backward Compatibility**: Option to generate old-style imports
